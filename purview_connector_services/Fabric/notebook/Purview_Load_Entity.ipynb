{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Purview Load Entity Notebook\n",
        "\n",
        "This notebook loads custom entities and types into Microsoft Purview from JSON files stored in Azure Data Lake Storage.\n",
        "\n",
        "## Prerequisites\n",
        "- Microsoft Fabric workspace with permissions to run notebooks\n",
        "- Azure Key Vault with secrets (automatically configured during deployment):\n",
        "  - `client-id`: Service principal application ID\n",
        "  - `client-secret`: Service principal password\n",
        "  - `tenant-id`: Azure Active Directory tenant ID\n",
        "- Service principal with:\n",
        "  - Storage Blob Data Contributor role on storage account\n",
        "  - Purview Data Curator role in Purview root collection\n",
        "\n",
        "## Configuration\n",
        "All sensitive credentials are retrieved from Azure Key Vault: `pccsakv6nvsfni5vtcj6`\n",
        "\n",
        "Update the cell below only if you need to change:\n",
        "- Storage account name\n",
        "- Purview account name\n",
        "- Container names\n",
        "- File paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required Python package for Purview integration\n",
        "%pip install pyapacheatlas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Configuration - Retrieved from Azure Key Vault for security\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "# Key Vault URI (deployed with your infrastructure)\n",
        "key_vault_uri = \"https://pccsakv6nvsfni5vtcj6.vault.azure.net/\"\n",
        "\n",
        "# Storage configuration\n",
        "blob_container_name = \"pccsa\"  # Container created during deployment\n",
        "blob_account_name = \"pccsast6nvsfni5vtcj6\"  # Your deployed storage account\n",
        "blob_relative_path = \"incoming\"  # Input files location\n",
        "blob_processed = \"processed\"  # Processed files location\n",
        "\n",
        "# Application configuration\n",
        "app_name = \"purviewspn\"\n",
        "purview_name = \"edinmedi-purview-labs\"\n",
        "\n",
        "# Retrieve secrets from Key Vault (more secure than hardcoding)\n",
        "TENANT_ID = mssparkutils.credentials.getSecret(key_vault_uri, \"tenant-id\")\n",
        "CLIENT_ID = mssparkutils.credentials.getSecret(key_vault_uri, \"client-id\")\n",
        "CLIENT_SECRET = mssparkutils.credentials.getSecret(key_vault_uri, \"client-secret\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import sys\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "\n",
        "# PyApacheAtlas packages\n",
        "# Connect to Atlas via a Service Principal\n",
        "from pyapacheatlas.auth import ServicePrincipalAuthentication\n",
        "from pyapacheatlas.core import PurviewClient, AtlasClassification, AtlasEntity, AtlasProcess, RelationshipTypeDef  # Communicate with your Atlas server\n",
        "from pyapacheatlas.readers import ExcelConfiguration, ExcelReader\n",
        "from pyapacheatlas.core.util import GuidTracker,AtlasException\n",
        "from pyapacheatlas.core import AtlasAttributeDef, AtlasEntity, PurviewClient\n",
        "from pyapacheatlas.core.typedef import EntityTypeDef\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "my_jars = os.environ.get(\"SPARK_HOME\")\n",
        "myconf = SparkConf()\n",
        "myconf.setMaster(\"local\").setAppName(app_name)\n",
        "myconf.set(\"spark.jars\",\"%s/jars/log4j-1.2.17.jar\" % my_jars)\n",
        "spark = SparkSession\\\n",
        " .builder\\\n",
        " .appName(app_name)\\\n",
        " .config(conf = myconf) \\\n",
        " .getOrCreate()\n",
        "\n",
        "\n",
        "Logger= spark._jvm.org.apache.log4j.Logger\n",
        "mylogger = Logger.getLogger(app_name)\n",
        "adls_home = 'abfss://%s@%s.dfs.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
        "adls_processed = 'abfss://%s@%s.dfs.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_processed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def log_msgs(msg_type,msg):\n",
        "        \n",
        "        if msg_type.upper() == \"ERROR\":\n",
        "            print('ERROR: %s' % repr(msg))\n",
        "            mylogger.error(repr(msg))\n",
        "        else:\n",
        "            print('INFO: %s' % repr(msg))\n",
        "            mylogger.info(repr(msg))\n",
        "           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "    # Authenticate against your Atlas server\n",
        "oauth = ServicePrincipalAuthentication(\n",
        "       tenant_id= TENANT_ID, \n",
        "       client_id=CLIENT_ID, \n",
        "       client_secret=CLIENT_SECRET \n",
        "  )\n",
        "client = PurviewClient(\n",
        "        account_name = os.environ.get(\"PURVIEW_NAME\", purview_name),\n",
        "        authentication=oauth\n",
        "    )\n",
        "gt = GuidTracker()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def Get_Rel_Inputs(json_obj):\n",
        "    try:\n",
        "        att_value = json_obj['relationshipAttributes']['inputs']\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def Get_Rel_Outputs(json_obj):\n",
        "    try:\n",
        "        att_value = json_obj['relationshipAttributes']['outputs']\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def Get_Outputs(json_obj):\n",
        "    try:\n",
        "        att_value = json_obj['attributes']['outputs']\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def Get_Inputs(json_obj):\n",
        "    try:\n",
        "        att_value = json_obj['attributes']['inputs']\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "def Get_Rel_Parents(json_obj):\n",
        "    try:\n",
        "        att_value = json_obj['relationshipAttributes']['parent']\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def search_entities_byQuilifiedName(name):\n",
        "    results = client.search_entities('qualifiedName\\:%s' % name.replace(\":\",\"\\:\").replace(\"/\",\"\\/\").replace(\"{\",\"\\{\").replace(\"}\",\"\\}\"))\n",
        "    guid = None\n",
        "    for result in results:\n",
        "        if result['qualifiedName'] == name:\n",
        "                guid= result['id']\n",
        "                if result['entityType'] != 'purview_custom_connector_generic_entity':\n",
        "                    #print(result)\n",
        "                    break\n",
        "    return guid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def removeDummyEntity(name):\n",
        "    try:\n",
        "        results = client.search_entities('qualifiedName\\:%s' % name.replace(\":\",\"\\:\").replace(\"/\",\"\\/\"))\n",
        "        for result in results:\n",
        "            if result['qualifiedName'] ==name:\n",
        "                entity = client.get_entity(guid=result['id'])\n",
        "                if len(entity['entities']) > 0 :\n",
        "                    if len(entity['entities'][0]['relationshipAttributes']['inputToProcesses']) == 0 and len(entity['entities'][0]['relationshipAttributes']['outputFromProcesses']) == 0:\n",
        "                        log_msgs(\"INFO\",'removeDummyEntity: Deleted Dummy entity%s' % (entity['entities'][0]))\n",
        "                        client.delete_entity(entity['entities'][0]['guid'])\n",
        "    except:\n",
        "        log_msgs(\"ERROR\",'Build_Entity_Json: %s' % (str(e)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def Create_Generic_Entity(dummy):\n",
        "    try:\n",
        "        log_msgs(\"INFO\",'Create_Generic_Entity - Dummy entity: %s' % dummy)\n",
        "        _qualifiedname = 'dummy://%s' % dummy['uniqueAttributes']['qualifiedName']\n",
        "        output_guid = search_entities_byQuilifiedName(_qualifiedname)\n",
        "        if output_guid==None:\n",
        "            attributes = {}\n",
        "            qualifiedName = dummy['uniqueAttributes']['qualifiedName']\n",
        "            attributes[\"purview_qualifiedName\"]= dummy['uniqueAttributes']['qualifiedName']\n",
        "            typeName = \"\"\n",
        "            if \"typeName\" in dummy['uniqueAttributes']:\n",
        "                typeName = dummy['uniqueAttributes']['typeName']\n",
        "            if \"source\" in dummy['uniqueAttributes']:\n",
        "                attributes[\"original_source\"]= dummy['uniqueAttributes']['source']\n",
        "            tmepname = qualifiedName.split('/')\n",
        "            Name= tmepname[len(tmepname)-1]\n",
        "            if \"Name\" in dummy['uniqueAttributes']:\n",
        "                Name = dummy['uniqueAttributes']['Name']\n",
        "            if \"name\" in dummy['uniqueAttributes']:\n",
        "                Name = dummy['uniqueAttributes']['name']\n",
        "\n",
        "            generic_entity = AtlasEntity(\n",
        "                name = Name,\n",
        "                qualified_name = _qualifiedname,\n",
        "                typeName = \"purview_custom_connector_generic_entity\",\n",
        "                attributes = attributes,\n",
        "                guid = gt.get_guid()\n",
        "                )\n",
        "            upload_results = client.upload_entities(batch=[generic_entity])\n",
        "            if 'mutatedEntities' in upload_results:\n",
        "                if 'CREATE' in upload_results['mutatedEntities']:\n",
        "                    if len(upload_results['mutatedEntities']['CREATE']) >0:\n",
        "                        log_msgs(\"INFO\",'Create_Generic_Entity: Entities Created/Updated')\n",
        "                        log_msgs(\"INFO\",'Create_Generic_Entity: %s' % upload_results)\n",
        "                        return upload_results['mutatedEntities']['CREATE'][0]['guid']\n",
        "                    else:\n",
        "                        log_msgs(\"ERROR\",'Create_Generic_Entity: Fail to retrieve gui')\n",
        "                        log_msgs(\"ERROR\",json.dump(upload_results))\n",
        "                        return None\n",
        "                else:\n",
        "                    log_msgs(\"ERROR\",'Create_Generic_Entity: Fail to retrieve CREATE')\n",
        "                    log_msgs(\"ERROR\",json.dump(upload_results))\n",
        "                    return None\n",
        "            log_msgs(\"ERROR\",'Create_Generic_Entity: Fail to retrieve mutatedEntities')\n",
        "            log_msgs(\"ERROR\", json.dump(upload_results))\n",
        "            return None\n",
        "        else:\n",
        "            return output_guid\n",
        "    except Exception as e:\n",
        "      log_msgs(\"ERROR\",'Create_Generic_Entity: %s' % (str(e)))\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def Load_Entity_Json(json_file):\n",
        "   try:\n",
        "    purview_load_entities=[]\n",
        "    for i in json_file:\n",
        "        json_obj = json.loads(i)\n",
        "        json_obj\n",
        "        purview_load_entities.append(json_obj)\n",
        "            \n",
        "    upload_results = client.upload_entities(batch=purview_load_entities)\n",
        "    log_msgs(\"INFO\",'Entities Created/Updated')\n",
        "    #print(json.dumps(upload_results, indent=2))\n",
        "    return True\n",
        "   except Exception as e:\n",
        "      log_msgs(\"ERROR\",'Load_Entity_Json: %s' % (str(e)))\n",
        "   return False\n",
        "\n",
        "def Load_Entity_Json_fromJson(_json):\n",
        "   try:\n",
        "    purview_load_entities=[]\n",
        "    for i in _json:\n",
        "        purview_load_entities.append(i)\n",
        "    \n",
        "    log_msgs(\"INFO\",(purview_load_entities))\n",
        "    upload_results = client.upload_entities(batch=purview_load_entities)\n",
        "    log_msgs(\"INFO\",'Entities Created/Updated')\n",
        "#    print(json.dumps(upload_results, indent=2))\n",
        "    return True\n",
        "   except Exception as e:\n",
        "      log_msgs(\"ERROR\",'Load_Entity_Json_fromJson: %s' % (str(e)))\n",
        "   return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def Build_Entity_Json(_json):\n",
        "    \n",
        "    _parent = gt.get_guid()\n",
        "    my_parents={}\n",
        "    final_json = []\n",
        "    rel_obj = {}\n",
        "    try:\n",
        "        for i in _json:\n",
        "            _qualifiedname=None\n",
        "            _typename = None\n",
        "            json_obj = json.loads(i)\n",
        "            try:\n",
        "                if 'typeName' in json_obj:\n",
        "                    _typename = json_obj['typeName']\n",
        "                if _typename == None:\n",
        "                    log_msgs('ERROR','JSON dont have typeName of the entity')\n",
        "                    return False\n",
        "            except:\n",
        "                log_msgs('ERROR','JSON dont have typeName of the entity')\n",
        "                return False\n",
        "            try:\n",
        "                _qualifiedname = json_obj['attributes']['qualifiedName']\n",
        "            except:\n",
        "                log_msgs('ERROR','JSON dont have attributes/qualifiedName of the entity')\n",
        "                return False\n",
        "\n",
        "            if _qualifiedname != None and _typename != None:\n",
        "                entity = client.get_entity(qualifiedName=_qualifiedname,typeName=_typename)\n",
        "                if len(entity) > 0:\n",
        "                    entity_guid = entity[\"entities\"][0][\"guid\"]\n",
        "                    print(entity_guid)\n",
        "                    json_obj['guid'] = entity_guid\n",
        "                    my_parents[_typename]=entity_guid\n",
        "                else:\n",
        "                    if not 'guid' in json_obj:\n",
        "                        json_obj['guid'] = gt.get_guid()\n",
        "                    my_parents[_typename]=json_obj['guid']\n",
        "\n",
        "                if Get_Outputs(json_obj):\n",
        "                    for each in json_obj['attributes']['outputs']:\n",
        "                        _qualifiedname = each['uniqueAttributes']['qualifiedName']\n",
        "                        dummyEntities.append('dummy://%s' % _qualifiedname)\n",
        "                        output_guid = search_entities_byQuilifiedName(_qualifiedname)\n",
        "                        if output_guid==None:\n",
        "                            refguid = Create_Generic_Entity(each) \n",
        "                            if refguid != None:\n",
        "                                each['uniqueAttributes']['guid'] = refguid\n",
        "                                rel_obj[_qualifiedname] = refguid\n",
        "                            else:\n",
        "                                log_msgs(\"ERROR\",'Build_Entity_Json - output - results: Can\\'t Create Dummy '.join(each))\n",
        "                                return False\n",
        "                        else:\n",
        "                            each['uniqueAttributes']['guid'] = output_guid\n",
        "                            rel_obj[_qualifiedname] = output_guid\n",
        "\n",
        "                if Get_Inputs(json_obj):\n",
        "                    for each in json_obj['attributes']['inputs']:\n",
        "                        _qualifiedname = each['uniqueAttributes']['qualifiedName']\n",
        "                        dummyEntities.append('dummy://%s' % _qualifiedname)\n",
        "                        input_guid = search_entities_byQuilifiedName(_qualifiedname)\n",
        "                        if input_guid==None:\n",
        "                            refguid = Create_Generic_Entity(each) \n",
        "                            if refguid != None:\n",
        "                                each['uniqueAttributes']['guid'] = refguid\n",
        "                                rel_obj[_qualifiedname] = refguid\n",
        "                            else:\n",
        "                                log_msgs(\"ERROR\",'Build_Entity_Json - Input - results: Can\\'t Create Dummy '.join(each))\n",
        "                                return False\n",
        "                        else:\n",
        "                            each['uniqueAttributes']['guid'] = input_guid\n",
        "                            rel_obj[_qualifiedname] = input_guid\n",
        "\n",
        "\n",
        "                if Get_Rel_Inputs(json_obj):\n",
        "                    for each in json_obj['relationshipAttributes']['inputs']:\n",
        "                        _qualifiedname = each['qualifiedName']\n",
        "                        each['guid'] = rel_obj[_qualifiedname]\n",
        "\n",
        "                if Get_Rel_Outputs(json_obj):\n",
        "                    for each in json_obj['relationshipAttributes']['outputs']:\n",
        "                        _qualifiedname = each['qualifiedName']\n",
        "                        each['guid'] = rel_obj[_qualifiedname]\n",
        "\n",
        "                if Get_Rel_Parents(json_obj):\n",
        "                    json_obj['relationshipAttributes']['parent']['guid'] = my_parents[json_obj['relationshipAttributes']['parent']['typeName']]\n",
        "                final_json.append(json_obj)\n",
        "            else:\n",
        "                return False\n",
        "    except Exception as e:\n",
        "        log_msgs(\"ERROR\",'Build_Entity_Json: %s' % (str(e)))\n",
        "        return False\n",
        "    try:\n",
        "        Load_Entity_Json_fromJson(final_json)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        log_msgs(\"ERROR\",'Build_Entity_Json: %s' % (str(e)))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "havefiles = True\n",
        "inicialnumfiles = 0\n",
        "dummyEntities = []\n",
        "while havefiles:\n",
        "    havefiles = False\n",
        "    files = mssparkutils.fs.ls(adls_home)\n",
        "    numoffiles = len(files)\n",
        "    processedfiles = 0\n",
        "    failfiles=0\n",
        "    for file in files:\n",
        "        if file.size > 0:\n",
        "            havefiles = True\n",
        "            i=0\n",
        "            filepath = \"\"\n",
        "            fileparts = file.path.split('/')\n",
        "            for filepart in fileparts:\n",
        "                if i < len(fileparts)-1:\n",
        "                    filepath+='%s/' % filepart\n",
        "                i+=1\n",
        "            \n",
        "            filepath='%s/%s' % (adls_processed,file.name)\n",
        "            load_json = False\n",
        "            readComplexJSONDF=None\n",
        "            try:\n",
        "                readComplexJSONDF = spark.read.option(\"multiLine\",\"true\").json(file.path)\n",
        "                load_json=True\n",
        "            except Exception as e:\n",
        "                log_msgs('ERROR','Invalid Json: %s /r %s' % file.path,e.args[0])\n",
        "\n",
        "            if load_json:\n",
        "                j = readComplexJSONDF.toJSON().collect()\n",
        "                log_msgs('INFO','Loading File: %s' % file.path)\n",
        "                if Build_Entity_Json(j):\n",
        "                    try:\n",
        "                        deletfile = mssparkutils.fs.rm(filepath)\n",
        "                        \n",
        "                    except:\n",
        "                        log_msgs('INFO','No file to delete')\n",
        "                    movefile = mssparkutils.fs.mv(src=file.path,dest=filepath)\n",
        "                    processedfiles+=1\n",
        "                else:\n",
        "                    failfiles+=1\n",
        "    if failfiles > 0  and processedfiles == 0:\n",
        "        print('Exit all files loaded')\n",
        "        break\n",
        "if len(dummyEntities) > 0:\n",
        "    for dummyEntitie in dummyEntities:\n",
        "        removeDummyEntity(dummyEntitie)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Fabric PySpark",
      "name": "fabric_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
